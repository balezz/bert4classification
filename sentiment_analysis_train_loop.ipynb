{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_train_loop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhwVKFyP5tL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d53624f-c4fd-43d2-b3d3-02a7bb9a1d3b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkkiXZYoT-W-"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5SDcGZWUDtW"
      },
      "source": [
        "train_path = '/drive/MyDrive/ml/sequora/datasets/internal_data/sentiment/sentiment_train.csv'\n",
        "valid_path = '/drive/MyDrive/ml/sequora/datasets/internal_data/sentiment/sentiment_valid.csv'\n",
        "test_path  = '/drive/MyDrive/ml/sequora/datasets/internal_data/sentiment/sentiment_test.csv'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwNPbOeVUkUo"
      },
      "source": [
        "train_data = pd.read_csv(train_path)\n",
        "valid_data = pd.read_csv(valid_path)\n",
        "test_data  = pd.read_csv(test_path)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wohN6PWPUq5R"
      },
      "source": [
        "train_data = train_data.drop(columns=['Unnamed: 0'])\n",
        "valid_data = valid_data.drop(columns=['Unnamed: 0'])\n",
        "test_data  = test_data.drop(columns=['Unnamed: 0'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQW5jH0cMT65",
        "outputId": "e064e034-8b18-4548-8d4d-38fea438fe36"
      },
      "source": [
        "train_data.columns"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK5Y-MUWl6Sx",
        "outputId": "e845e19c-42dd-44ec-994b-33cfc7289fb3"
      },
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba9c0MVgl6VZ"
      },
      "source": [
        "import torch\n",
        "# from transformers import AutoTokenizer, AutoModel"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QypRoPQwzdUy"
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiC7iDsWmBe-",
        "outputId": "d5124a9f-6bfd-470e-d2ec-c9a0a4092043"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2STn7GK21rNn"
      },
      "source": [
        "model.classifier = torch.nn.Linear(312, 3)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz94EO40_btp"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JboTatN-_XxX",
        "outputId": "44908486-32fd-4522-8ea1-b792e68ba145"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(29564, 312, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 312)\n",
              "      (token_type_embeddings): Embedding(2, 312)\n",
              "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
              "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
              "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
              "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
              "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=312, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoJU6cfvNAuW"
      },
      "source": [
        "# Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, texts, targets, tokenizer, max_len=256):\n",
        "    self.texts = texts\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.texts[idx])\n",
        "    target = self.targets[idx]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'text': text,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFLZpye9VmU"
      },
      "source": [
        "train_set = CustomDataset(\n",
        "    texts=list(train_data['text']),\n",
        "    targets=list(train_data['label']),\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21ZGvfnXBR2r"
      },
      "source": [
        "valid_set = CustomDataset(\n",
        "    texts=list(valid_data['text']),\n",
        "    targets=list(valid_data['label']),\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50ICSJYOKDbY"
      },
      "source": [
        "test_set = CustomDataset(\n",
        "    texts=list(test_data['text']),\n",
        "    targets=list(test_data['label']),\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d70GDzkOO_KL"
      },
      "source": [
        "# DataLoader\n",
        "train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6-zVGsLBO-7"
      },
      "source": [
        "valid_loader = DataLoader(\n",
        "        valid_set,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx6px_BQKHNh"
      },
      "source": [
        "test_loader = DataLoader(\n",
        "        test_set,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ6qnu3Q-yvR"
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHchmr14-z7W"
      },
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvGeHCm9-z9t"
      },
      "source": [
        "total_steps = len(train_loader) * EPOCHS"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcdlJKUi-0AL"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-XqgIh0PAbd"
      },
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UFIpdzW_T8q"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def train_epoch(\n",
        "    model,\n",
        "    data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    n_examples\n",
        "    ):\n",
        "\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"targets\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        loss = loss_fn(outputs.logits, targets)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b2qp8RLAI7Z"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            loss = loss_fn(outputs.logits, targets)\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuqnAMn4Al7R",
        "outputId": "64ebb1aa-3aae-4c8a-81d5-2cc5a107d4ff"
      },
      "source": [
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        len(train_data)\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        valid_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(valid_data)\n",
        "      )\n",
        "\n",
        "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "        torch.save(model, '/drive/MyDrive/ml/sequora/sentiment/model/sent_bert.pt')\n",
        "        best_accuracy = val_acc"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5245558921321145 accuracy 0.8816385343762866\n",
            "Val loss 0.3511570269340001 accuracy 0.9282325029655991\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 0.391467561904569 accuracy 0.9186908192671881\n",
            "Val loss 0.2939057175957824 accuracy 0.9454329774614473\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 0.291646966267689 accuracy 0.9425689584191025\n",
            "Val loss 0.2148947766041582 accuracy 0.9612495057334915\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 0.20362107071837554 accuracy 0.962536023054755\n",
            "Val loss 0.1299458675006126 accuracy 0.9754843811783314\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.16139746729072435 accuracy 0.9715932482503088\n",
            "Val loss 0.11157251601026229 accuracy 0.9816132858837485\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.11382813374177335 accuracy 0.9796212433100041\n",
            "Val loss 0.08122409733226932 accuracy 0.9859628311585608\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.08462926000179344 accuracy 0.9839440098806093\n",
            "Val loss 0.08149966069587805 accuracy 0.9875444839857651\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.055582685270760435 accuracy 0.9880609304240429\n",
            "Val loss 0.0684553126542576 accuracy 0.990707789640174\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.04024773038524737 accuracy 0.9919720049403047\n",
            "Val loss 0.0659708488299365 accuracy 0.9911032028469751\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.03984568732345261 accuracy 0.992383696994648\n",
            "Val loss 0.0659708490012955 accuracy 0.9911032028469751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzQmVs0VBtLE",
        "outputId": "b493013d-0f4e-4a0d-f6ce-c8f13b397b15"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(test_data)\n",
        ")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4J2mdzKKXzF",
        "outputId": "7baa6eba-08d1-466f-8b44-7f04d8f84238"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8600, device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPI_0E4YKoT0"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "    model = model.eval()\n",
        "    target_texts = []\n",
        "    predictions = []\n",
        "    real_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            texts = d[\"text\"]\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            target_texts.extend(texts)\n",
        "            predictions.extend(preds)\n",
        "\n",
        "            real_values.extend(targets)\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu()\n",
        "    real_values = torch.stack(real_values).cpu()\n",
        "\n",
        "    return target_texts, predictions, real_values"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "946fE4uxLg8g",
        "outputId": "8a91476e-16da-4073-8a5b-1aea4c5d1113"
      },
      "source": [
        "target_texts, predictions, real_values = get_predictions(model, test_loader)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ucXroKJA29i"
      },
      "source": [
        "model = torch.load('/drive/MyDrive/ml/sequora/sentiment/model/sent_bert.pt')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0auQ1HnxBIps"
      },
      "source": [
        "torch.save(model, '/drive/MyDrive/ml/sequora/sentiment/model/sent_bert.pt')"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noEwyqmINaDP"
      },
      "source": [
        "https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n",
        "\n",
        "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n",
        "\n",
        "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
      ]
    }
  ]
}